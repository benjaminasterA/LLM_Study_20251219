1
00:00:00,000 --> 00:00:04,437
대형 언어 모델(LLM)은 다음과 같은 방식으로 동작합니다:

1.

2
00:00:04,437 --> 00:00:04,850
**데이터 수집**: 다양한 출처에서 텍스트 데이터를 수집합니다.

3
00:00:04,850 --> 00:00:05,337
2.

4
00:00:05,337 --> 00:00:11,274
**토큰화**: 텍스트를 작은 단위(토큰)로 나누어 모델이 이해할 수 있도록 변환합니다.

5
00:00:11,274 --> 00:00:11,687
3.

6
00:00:11,687 --> 00:00:15,024
**모델 훈련**: 신경망을 통해 대량의 데이터를 학습합니다.

7
00:00:15,024 --> 00:00:22,187
주로 Transformer 아키텍처를 사용하며, 입력에 대해 다음 단어를 예측하는 방식으로 학습합니다.

8
00:00:22,187 --> 00:00:22,587
4.

9
00:00:22,587 --> 00:00:27,625
**인퍼런스**: 학습된 모델을 사용해 새로운 입력에 대해 응답을 생성합니다.

10
00:00:27,625 --> 00:00:33,112
이 과정에서 입력 텍스트에 기반하여 다음 단어를 단계적으로 생성합니다.

11
00:00:33,112 --> 00:00:33,574
5.

12
00:00:33,574 --> 00:00:37,749
**출력 생성**: 생성된 토큰을 결합하여 최종 문장을 만들어냅니다.

13
00:00:37,749 --> 00:00:47,124
이 모든 과정은 데이터를 기반으로 패턴을 이해하고, 언어의 맥락을 고려하여 자연스러운 텍스트를 만드는 데 중점을 둡니다.

